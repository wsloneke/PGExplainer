{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./codes/')\n",
    "from config import args\n",
    "\n",
    "args.dataset='syn3'\n",
    "args.elr = 0.003\n",
    "args.eepochs = 20\n",
    "args.coff_size = 0.0001\n",
    "args.budget = -1\n",
    "args.coff_ent = 0.01\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from utils import *\n",
    "from models import GCN\n",
    "from metrics import *\n",
    "import numpy as np\n",
    "from Extractor import Extractor\n",
    "from Explainer import Explainer\n",
    "from scipy.sparse import coo_matrix,csr_matrix\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './dataset/syn3.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./dataset/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m      2\u001b[0m     adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, edge_label_matrix  \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(fin)\n\u001b[0;32m      4\u001b[0m adj \u001b[38;5;241m=\u001b[39m csr_matrix(adj)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './dataset/syn3.pkl'"
     ]
    }
   ],
   "source": [
    "with open('./dataset/' + args.dataset + '.pkl', 'rb') as fin:\n",
    "    adj, features, y_train, y_val, y_test, train_mask, val_mask, test_mask, edge_label_matrix  = pkl.load(fin)\n",
    "\n",
    "adj = csr_matrix(adj)\n",
    "support = preprocess_adj(adj)\n",
    "\n",
    "features_tensor = tf.convert_to_tensor(features,dtype=tf.float32)\n",
    "support_tensor = tf.SparseTensor(*support)\n",
    "support_tensor = tf.cast(support_tensor,tf.float32)\n",
    "\n",
    "model = GCN(input_dim=features.shape[1], output_dim=y_train.shape[1])\n",
    "model.load_weights(args.save_path+ args.dataset)\n",
    "\n",
    "explainer = Explainer(model=model)\n",
    "embeds = model.embedding((features_tensor,support_tensor)).numpy()\n",
    "\n",
    "all_label = np.logical_or(y_train,np.logical_or(y_val,y_test))\n",
    "single_label = np.argmax(all_label,axis=-1)\n",
    "hops = len(args.hiddens.split('-'))\n",
    "extractor = Extractor(adj,features,edge_label_matrix,embeds,all_label,hops)\n",
    "if args.setting==1:\n",
    "    if args.dataset=='syn3':\n",
    "        allnodes = [i for i in range(511,871,6)]\n",
    "    elif args.dataset=='syn4':\n",
    "        allnodes = [i for i in range(511,800,1)]\n",
    "    else:\n",
    "        allnodes = [i for i in range(400,700,5)] # setting from their original paper\n",
    "elif args.setting==2:\n",
    "    allnodes = [i for i in range(single_label.shape[0]) if single_label[i] ==1]\n",
    "elif args.setting==3:\n",
    "    if args.dataset == 'syn2':\n",
    "        allnodes = [i for i in range(single_label.shape[0]) if single_label[i] != 0 and single_label[i] != 4]\n",
    "    else:\n",
    "        allnodes = [i for i in range(single_label.shape[0]) if single_label[i] != 0]\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=args.elr)\n",
    "clip_value_min = -2.0\n",
    "clip_value_max = 2.0\n",
    "\n",
    "sub_support_tensors = []\n",
    "sub_label_tensors = []\n",
    "sub_features = []\n",
    "sub_embeds = []\n",
    "sub_adjs = []\n",
    "sub_edge_labels = []\n",
    "sub_labels = []\n",
    "remap = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in allnodes:\n",
    "    sub_adj,sub_feature, sub_embed, sub_label,sub_edge_label_matrix = extractor.subgraph(node)\n",
    "    remap[node]=len(sub_adjs)\n",
    "    sub_support = preprocess_adj(sub_adj)\n",
    "    sub_support_tensor = tf.cast(tf.SparseTensor(*sub_support),tf.float32)\n",
    "    sub_label_tensor = tf.convert_to_tensor(sub_label,dtype=tf.float32)\n",
    "\n",
    "    sub_adjs.append(sub_adj)\n",
    "    sub_features.append(tf.cast(sub_feature,tf.float32))\n",
    "    sub_embeds.append(sub_embed)\n",
    "    sub_labels.append(sub_label)\n",
    "    sub_edge_labels.append(sub_edge_label_matrix)\n",
    "    sub_label_tensors.append(sub_label_tensor)\n",
    "    sub_support_tensors.append(sub_support_tensor)\n",
    "best_auc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot(node,label):\n",
    "    after_adj_dense = explainer.masked_adj.numpy()\n",
    "    after_adj = coo_matrix(after_adj_dense)\n",
    "\n",
    "    rcd = np.concatenate([np.expand_dims(after_adj.row,-1),np.expand_dims(after_adj.col,-1),np.expand_dims(after_adj.data,-1)],-1)\n",
    "    pos_edges = []\n",
    "    filter_edges = []\n",
    "    edge_weights = after_adj.data\n",
    "    sorted_edge_weights = np.sort(edge_weights)\n",
    "    thres_index = max(int(edge_weights.shape[0]-12),0)\n",
    "    thres = sorted_edge_weights[thres_index]\n",
    "    filter_thres_index = min(thres_index,max(int(edge_weights.shape[0]-edge_weights.shape[0]/2),edge_weights.shape[0]-100))\n",
    "    # filter_thres_index = min(thres_index,max(int(edge_weights.shape[0]-edge_weights.shape[0]/4),edge_weights.shape[0]-100))\n",
    "    filter_thres = sorted_edge_weights[filter_thres_index]\n",
    "    filter_nodes =set()\n",
    "\n",
    "    # print(sorted_edge_weights)\n",
    "#     print('thres',thres)\n",
    "    for r,c,d in rcd:\n",
    "        r = int(r)\n",
    "        c = int(c)\n",
    "        if d>=thres:\n",
    "            pos_edges.append((r,c))\n",
    "        if d>filter_thres:\n",
    "            filter_edges.append((r,c))\n",
    "            filter_nodes.add(r)\n",
    "            filter_nodes.add(c)\n",
    "\n",
    "    num_nodes = sub_adj.shape[0]\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(num_nodes))\n",
    "    G.add_edges_from(filter_edges)\n",
    "\n",
    "    for cc in nx.connected_components(G):\n",
    "        if 0 in cc:\n",
    "            G = G.subgraph(cc).copy()\n",
    "            break\n",
    "\n",
    "    pos_edges = [(u, v) for (u, v) in pos_edges if u in G.nodes() and v in G.nodes()]\n",
    "    pos = nx.kamada_kawai_layout(G)\n",
    "\n",
    "    colors = ['orange', 'red', 'green', 'blue', 'maroon', 'brown', 'darkslategray', 'paleturquoise', 'darksalmon',\n",
    "              'slategray', 'mediumseagreen', 'mediumblue', 'orchid', ]\n",
    "    if args.dataset=='syn3':\n",
    "        colors = ['orange', 'blue']\n",
    "\n",
    "\n",
    "    if args.dataset=='syn4':\n",
    "        colors = ['orange', 'black','black','black','blue']\n",
    "\n",
    "\n",
    "    # nodes\n",
    "    labels = label#.numpy()\n",
    "    max_label = np.max(labels)+1\n",
    "#     print(max_label)\n",
    "\n",
    "    nmb_nodes = after_adj_dense.shape[0]\n",
    "    label2nodes= []\n",
    "    for i in range(max_label):\n",
    "    \tlabel2nodes.append([])\n",
    "    for i in range(nmb_nodes):\n",
    "    \tlabel2nodes[labels[i]].append(i)\n",
    "\n",
    "    for i in range(max_label):\n",
    "        node_filter = []\n",
    "        for j in range(len(label2nodes[i])):\n",
    "            if label2nodes[i][j] in G.nodes():\n",
    "                node_filter.append(label2nodes[i][j])\n",
    "        nx.draw_networkx_nodes(G, pos,\n",
    "                               nodelist=node_filter,\n",
    "                               node_color=colors[i % len(colors)],\n",
    "                               node_size=500)\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos,\n",
    "                           nodelist=[0],\n",
    "                           node_color=colors[labels[0]],\n",
    "                           node_size=1000)\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos, width=7, alpha=0.5, edge_color='grey')\n",
    "\n",
    "    nx.draw_networkx_edges(G, pos,\n",
    "                           edgelist=pos_edges,\n",
    "                           width=7, alpha=0.5)\n",
    "\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "#     plt.savefig('./vis/'+args.dataset+'/P'+str(node)+'.png')\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print('extract sub graphs done')\n",
    "    t0 = args.coff_t0\n",
    "    t1 = args.coff_te\n",
    "    epochs = args.eepochs\n",
    "    for epoch in range(epochs):\n",
    "        train_accs = []\n",
    "        loss = 0\n",
    "        pred_loss = 0\n",
    "        lap_loss = 0\n",
    "        tmp = float(t0*np.power(t1/t0,epoch/epochs))\n",
    "        tmp = 5.0\n",
    "        with tf.GradientTape() as tape:\n",
    "            for i in range(len(allnodes)):\n",
    "                output = model.call((sub_features[i],sub_support_tensors[i]),training=False)\n",
    "                train_acc = accuracy(output, sub_label_tensors[i])\n",
    "                train_accs.append(float(train_acc))\n",
    "                pred_label = tf.argmax(output, 1)\n",
    "\n",
    "                x = sub_features[i]\n",
    "                adj = sub_adjs[i]\n",
    "                nodeid = 0\n",
    "                embed = sub_embeds[i]\n",
    "                pred = explainer((x,adj,nodeid,embed,tmp),training=True)\n",
    "                l,pl,ll = explainer.loss(pred, pred_label, sub_label_tensor, 0)\n",
    "                loss += l\n",
    "                pred_loss += pl\n",
    "                lap_loss += ll\n",
    "            train_variables = []\n",
    "            for para in explainer.trainable_variables:\n",
    "                if para.name.startswith('explainer'):\n",
    "                    train_variables.append(para)\n",
    "        grads = tape.gradient(loss, train_variables)\n",
    "        cliped_grads = [tf.clip_by_value(t, clip_value_min, clip_value_max) for t in grads]\n",
    "        optimizer.apply_gradients(zip(cliped_grads, train_variables))\n",
    "\n",
    "        print('pred_loss_mean',np.mean(pred_loss.numpy()))\n",
    "\n",
    "        explainer.save_weights(args.save_path + 'expaliner_'+args.dataset)\n",
    "\n",
    "\n",
    "reals = []\n",
    "preds = []\n",
    "def acc(sub_adj,sub_edge_label):\n",
    "    real = []\n",
    "    pred = []\n",
    "    sub_edge_label = sub_edge_label.todense()\n",
    "    mask = explainer.masked_adj.numpy()\n",
    "    for r,c in list(zip(sub_adj.row,sub_adj.col)):\n",
    "        d = sub_edge_label[r,c] + sub_edge_label[c,r]\n",
    "        if d==0:\n",
    "            real.append(0)\n",
    "        else:\n",
    "            real.append(1)\n",
    "        pred.append(mask[r][c]+mask[c][r])\n",
    "    reals.extend(real)\n",
    "    preds.extend(pred)\n",
    "\n",
    "    if len(np.unique(real))==1 or len(np.unique(pred))==1:\n",
    "        return -1\n",
    "    return roc_auc_score(real,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_test(node,needplot=True):\n",
    "    newid = remap[node]\n",
    "    sub_adj, sub_feature, sub_embed, sub_label, sub_edge_label =  sub_adjs[newid],sub_features[newid],sub_embeds[newid],sub_labels[newid],sub_edge_labels[newid]\n",
    "\n",
    "    nodeid = 0\n",
    "    explainer((sub_feature,sub_adj,nodeid,sub_embed,1.0),training=False)\n",
    "    label = np.argmax(sub_label,-1)\n",
    "    if needplot:\n",
    "        plot(node,label)\n",
    "    acc(sub_adj,sub_edge_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer.load_weights(args.save_path + 'expaliner_'+args.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reals= []\n",
    "preds = []\n",
    "for node in allnodes:\n",
    "    explain_test(node,needplot=False)\n",
    "    auc = roc_auc_score(reals, preds)\n",
    "    print('node ',node, 'auc',auc)\n",
    "\n",
    "print('mean acc',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_test(allnodes[1],needplot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
